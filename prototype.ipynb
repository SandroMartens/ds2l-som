{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from minisom import MiniSom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Paper:  \n",
    "\"Enriched topological learning for cluster detection and visualization\"  \n",
    "Guénaël Cabanes, Younès Bennani, Dominique Fresneau  \n",
    "10.1016/j.neunet.2012.02.019 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_X = load_digits().images.reshape(1797, -1)\n",
    "mnist_y = load_digits().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prototypes(data, SOM_dim=None):\n",
    "    \"\"\"Define SOM and train on data.\n",
    "\n",
    "    Input:\n",
    "        - Data, SOM dimensions\n",
    "\n",
    "    Output: \n",
    "        - Trained SOM Object\n",
    "    \"\"\"\n",
    "    SOM = MiniSom(SOM_dim, SOM_dim, data.shape[1])\n",
    "    SOM.pca_weights_init(data)\n",
    "    SOM.train(data=data, num_iteration=100000)\n",
    "    return SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mnist_X\n",
    "SOM_dim = 15\n",
    "\n",
    "if SOM_dim is None:\n",
    "    sample_size = len(data)\n",
    "    #  n_neurons = 5 * sqrt(sample_size)\n",
    "    #  dim = sqrt(n_neurons)\n",
    "    SOM_dim = int((5*(sample_size**(1/2)))**(1/2))\n",
    "\n",
    "SOM = get_prototypes(data, SOM_dim=SOM_dim)\n",
    "Dist = SOM._distance_from_weights(data).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_prototypes(Dist, sigma=None):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        - Distance Matrix Dist(w, x) between M prototypes w und the N data x\n",
    "\n",
    "    Output: \n",
    "        - The density D_i and local variability s_i associated to each prototype w_i,  \n",
    "        - The neighborhood values v_{i,j} associated with each pair of prototypes w_i and w_j\n",
    "    \"\"\"\n",
    "    d = estimate_density(Dist=Dist, sigma=sigma, SOM=SOM)\n",
    "    s = estimate_local_variability(Dist=Dist, SOM=SOM)\n",
    "    v = estimate_neighborhood_values(Dist)\n",
    "\n",
    "    prototypes = pd.DataFrame([s, d], index=[\"s\", \"d\"]).T\n",
    "    v = pd.DataFrame(v)\n",
    "\n",
    "    return v, prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D_i = 1/N * \\sum_{k=1}^N \\frac{e^{-\\frac{{Dist(w_i, x_k)^2}}{2\\sigma^2}}}{\\sigma \\sqrt{2\\pi}}$  \n",
    "$D_i =  \\frac{1/N }{\\sigma \\sqrt{2\\pi}} * \\sum_{k=1}^N e^{-\\frac{{Dist(w_i, x_k)^2}}{2\\sigma^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_density(Dist, SOM, sigma=None):\n",
    "    \"\"\"Estimate local density for each prototype from its assigned samples.\"\"\"\n",
    "    #  Default value: Average distance between prototype and nearest neighbor.\n",
    "    if sigma is None:\n",
    "        sigma = Dist.min(axis=1).mean()\n",
    "\n",
    "    indices = SOM.win_map(data, return_indices=True)\n",
    "    D =  np.zeros(shape=(SOM_dim, SOM_dim))\n",
    "\n",
    "    for prototype_index, samples_indices in indices.items():\n",
    "        index_flat = np.ravel_multi_index(prototype_index, (SOM_dim, SOM_dim))\n",
    "        neighbors = Dist[index_flat, samples_indices]\n",
    "        # print(neighbors)\n",
    "        neighbors = neighbors**2\n",
    "        neighbors = np.e ** -(neighbors/(2*sigma**2))\n",
    "        neighbors = neighbors / sigma*np.sqrt(2*np.pi)\n",
    "        neighbors = np.mean(neighbors)\n",
    "\n",
    "        D[prototype_index] = np.mean(neighbors)\n",
    "\n",
    "    return D.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_neighborhood_values(Dist):\n",
    "    \"\"\"For each data x, find the two clostest prototypes u*(x) and u**(x) (Best Matching Units, BMUs).\n",
    "\n",
    "    Compute the number v_{i,j} of data having i and j as first two BMUs\n",
    "    \"\"\"\n",
    "    BMUs = np.argsort(Dist, axis=0)[:2,:]\n",
    "    v = np.zeros(shape=(SOM_dim**2, SOM_dim**2))\n",
    "    u, counts = np.unique(BMUs, axis=1, return_counts=True)\n",
    "    u = u.T\n",
    "    for index, combination in enumerate(u):\n",
    "        v[combination[0], combination[1]] = counts[index]\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_local_variability(Dist, SOM):\n",
    "    \"\"\"For each prototype w, variability s is the mean distance \n",
    "    between w and the L data x_w represented by w.\n",
    "\n",
    "    s_i = 1/L \\sum(j=1..L) Dist(w_i, x_wi)\n",
    "    \"\"\"\n",
    "    indices = SOM.win_map(data, return_indices=True)\n",
    "    s = np.zeros(shape=(SOM_dim, SOM_dim))\n",
    "\n",
    "    for prototype_index, samples_indices in indices.items():\n",
    "        index_flat = np.ravel_multi_index(prototype_index, (SOM_dim, SOM_dim))\n",
    "        s[prototype_index] = Dist[index_flat, samples_indices].mean()\n",
    "\n",
    "    return s.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, prototypes = enrich_prototypes(Dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_connected_units(v, threshold):\n",
    "    \"\"\"Find all L groups of linked prototypes i, j where v_{i,j} > treshold.\n",
    "    Input:\n",
    "        - v: Matrix of connected components (v_{i,j} have common samples).\n",
    "    \"\"\"\n",
    "    indices = np.where(v > threshold)\n",
    "    groups = set()\n",
    "    for i in zip(indices[0], indices[1]):\n",
    "        groups.add((i))\n",
    "    groups = pd.DataFrame(groups)\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_list = extract_connected_units(v, threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(groups, prototypes):\n",
    "    \"\"\"Create Graph with edges between prototypes. Edges are directed from\n",
    "    high density nodes to low density nodes with a positive gradient.\n",
    "\n",
    "    Input:\n",
    "        - The list of connected prototypes,\n",
    "        - Data for each prototype\n",
    "    Output:\n",
    "        - Graph object with protoypes and gradients between prototypes\n",
    "    \"\"\"\n",
    "    for i in range(len(groups)):\n",
    "        #  Negative gradient for Label Propagation\n",
    "        groups.loc[i, \"gradient\"] = -(prototypes.d[groups.loc[i, 0]] - prototypes.d[groups.loc[i, 1]])\n",
    "\n",
    "    positive_edges = groups[groups.gradient > 0]\n",
    "    #  reverse edge direction for later algorithms\n",
    "    G = nx.from_pandas_edgelist(positive_edges, source=1, target=0, edge_attr=\"gradient\", create_using=nx.DiGraph)\n",
    "\n",
    "    nx.set_node_attributes(G, prototypes.d, \"density\")\n",
    "    nx.set_node_attributes(G, prototypes.s, \"variability\")\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_graph(neighborhood_list, prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_clustering(G):\n",
    "    \"\"\"Label each prototype by the maximum with the highest gradient.\n",
    "\n",
    "    Input: \n",
    "        - Graph\n",
    "\n",
    "    Output:\n",
    "        - None\n",
    "    \"\"\"\n",
    "    #  Create initial labels\n",
    "    for node in G:\n",
    "        G.nodes[node][\"label\"] = node\n",
    "\n",
    "    #  Number of needed iterations\n",
    "    longest_path = nx.algorithms.dag.dag_longest_path_length(G)\n",
    "\n",
    "    for i in range(longest_path):\n",
    "        #  Iterate over (node, neighbor) pairs\n",
    "        for node, edges in G.pred.items():\n",
    "\n",
    "            #  get largest gradient neighbor\n",
    "            largest_gradient = 0\n",
    "            largest_gradient_neighbor = node\n",
    "            for edge in edges.items():\n",
    "                current_neighbor = edge[0]\n",
    "                current_gradient = edge[1][\"gradient\"]\n",
    "\n",
    "                if current_gradient > largest_gradient:\n",
    "                    largest_gradient = current_gradient\n",
    "                    largest_gradient_neighbor = current_neighbor\n",
    "\n",
    "            G.nodes[node][\"label\"] = G.nodes[largest_gradient_neighbor][\"label\"]\n",
    "\n",
    "initial_clustering(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(G.nodes(data=\"label\"))\n",
    "labels = set(labels.loc[:,1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maxima(G):\n",
    "    \"\"\"Find all Nodes which only have positive (negative internally) outgoing gradients.\n",
    "    ToDo: Check signs\n",
    "\n",
    "    Input: \n",
    "        - Graph\n",
    "\n",
    "    Output:\n",
    "        - List of maxima nodes\n",
    "    \"\"\"\n",
    "    maxima = []\n",
    "    for node in G:\n",
    "        if G.in_degree(node) == 0:\n",
    "            maxima.append(node)\n",
    "\n",
    "    return maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxima = find_maxima(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(G):\n",
    "    \"\"\"Check if gradients in the edge weights are the same as differences between gradients stored in nodes.\n",
    "\n",
    "    Input:\n",
    "        - Graph\n",
    "    \"\"\"\n",
    "    for node, nbrs in G.adj.items():\n",
    "        for nbr, edge_atrr in nbrs.items():\n",
    "            gradient =  G.nodes[nbr][\"density\"] - G.nodes[node][\"density\"]\n",
    "            if edge_atrr[\"gradient\"] != -gradient:\n",
    "                print(\"Wrong gradient found\")\n",
    "\n",
    "check_gradients(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [3, 7, 138, 10, 142, 16, 144, 149, 150, 154, 28, 30, 167, 39, 170, 173, 175, 177, 183, 58, 189, 62, 191, 64, 65, 194, 67, 201, 202, 74, 207, 81, 213, 85, 91, 222, 224, 98, 108, 114, 116, 121, 125]\n",
      "Number of labels: 43\n"
     ]
    }
   ],
   "source": [
    "print(\"labels\", list(set([val[1] for val in G.nodes.data(\"label\")])))\n",
    "n_labels = len(list(set([val[1] for val in G.nodes.data(\"label\")])))\n",
    "print(f\"Number of labels: {n_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_micro_clusters(G, label_i, label_j, density_i, density_j):\n",
    "    #  Get larger density label\n",
    "    if density_i > density_j:\n",
    "        new_label = label_i\n",
    "        old_label = label_j\n",
    "    else:\n",
    "        new_label = label_j\n",
    "        old_label = label_i\n",
    "\n",
    "    #  Overwrite lower density label\n",
    "    for node, label in G.nodes.data(\"label\"):\n",
    "        if label == old_label:\n",
    "            G.nodes[node][\"label\"] = new_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_clustering(G):\n",
    "    \"\"\"Merge clusters according to density threshold of the clusters.\n",
    "    \n",
    "    Input:\n",
    "        - Graph\n",
    "    \"\"\"\n",
    "    for e in G.edges:\n",
    "        node_i = e[0]\n",
    "        node_j = e[1]\n",
    "\n",
    "        label_i = G.nodes[node_i][\"label\"]\n",
    "        label_j = G.nodes[node_j][\"label\"]\n",
    "\n",
    "        density_i = G.nodes[node_i][\"density\"]\n",
    "        density_j = G.nodes[node_j][\"density\"]\n",
    "\n",
    "        density_max_i = G.nodes[label_i][\"density\"]\n",
    "        density_max_j = G.nodes[label_j][\"density\"]\n",
    "\n",
    "        threshold = (1/density_max_i + 1/density_max_j)**-1\n",
    "\n",
    "        if (density_i > threshold and \n",
    "            density_j > threshold and\n",
    "            label_i != label_j):\n",
    "                merge_micro_clusters(G, label_i, label_j, density_i, density_j)\n",
    "\n",
    "final_clustering(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [3, 7, 74, 175, 62, 191]\n",
      "Number of labels: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"labels\", list(set([val[1] for val in G.nodes.data(\"label\")])))\n",
    "n_labels = len(list(set([val[1] for val in G.nodes.data(\"label\")])))\n",
    "print(f\"Number of labels: {n_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>213</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>108</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>52</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>56</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1\n",
       "0    213  62\n",
       "1    198  62\n",
       "2     30   3\n",
       "3     15   3\n",
       "4    115  62\n",
       "..   ...  ..\n",
       "203  108  62\n",
       "204   52  62\n",
       "205   56  62\n",
       "206    5   3\n",
       "207   62  62\n",
       "\n",
       "[208 rows x 2 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([val for val in G.nodes.data(\"label\")])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbff9bfab3ee0df4124016b4e010029cf2ec5864ce5e3c4aa09796cc364af95f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
